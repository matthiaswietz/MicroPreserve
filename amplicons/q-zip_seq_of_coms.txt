

#source parameter file
. ./q-zip_parameters_[Euks|Pros].txt
#please adjust: q-zip_parameters_Euks.txt or q-zip_parameters_Pros.txt


#create needed directory for reference sequences
mkdir ./refDB/

#Search for primers in reference and trim primers:
cat ${REF_RAW_PATH}"/"${REF_DBS}".fasta" | ${CUTADAPT} -g ${FORWARDPRIMER} --discard-untrimmed --minimum-length ${MIN_LEN_REF} -e ${PRIMER_MISMATCH_REF} -O ${lenFP_CUT_REF} - | ${CUTADAPT} -a ${REVERSEPRIMER_RC} --discard-untrimmed --minimum-length ${MIN_LEN_REF} --maximum-length ${MAX_LEN_REF} -e ${PRIMER_MISMATCH_REF} -O ${lenRP_CUT_REF} - > ${REF_USED_FP}"/"${REF_DBS}"_"${FORWARDPRIMER}"_"${REVERSEPRIMER_RC}"_"${PRIMER_MISMATCH_REF}"_"${lenFP_CUT_REF}"_"${lenRP_CUT_REF}".fasta"

#prepare corresponding taxonomy map:
grep ">" ${REF_USED_FP}"/"${REF_DBS}"_"${FORWARDPRIMER}"_"${REVERSEPRIMER_RC}"_"${PRIMER_MISMATCH_REF}"_"${lenFP_CUT_REF}"_"${lenRP_CUT_REF}".fasta" | tr -d "^>" | awk 'FNR==NR{a[$1]=$1; next}($1 in a){print $0}' - ${REF_RAW_PATH}"/"${REF_DBS}".tax" > ${REF_USED_FP}"/"${REF_DBS}"_"${FORWARDPRIMER}"_"${REVERSEPRIMER_RC}"_"${PRIMER_MISMATCH_REF}"_"${lenFP_CUT_REF}"_"${lenRP_CUT_REF}".tax"

#prepare sample name mapping file:
ls -1 | grep "001.fastq.gz$\|001.fastq$" | awk '{if ($0 ~ /_R1_/) {printf($0); gsub(/_L00[1-4]_R1_001.fastq.gz/,"",$0); gsub(/_L00[1-4]_R1_001.fastq/,"",$0) ; printf("\t"$0"\t")} else {print $0}}' | awk '{print($2"\t"$1"\t"$3)}' > ${PROJECT_ID}".map"

#3'-end trimming:
cut -f 1 ${PROJECT_ID}".map" | grep -v "^#" |tr "." "_" | parallel -j ${THREADS} java -jar ${TRIMMOMATIC}" PE -phred33 {}_L001_R1_001.fastq* {}_L001_R2_001.fastq* {.}.trimmed.R1 /dev/null {.}.trimmed.R2 /dev/null CROP:"${CROP}" SLIDINGWINDOW:"${SLIDINGWINDOW}

#paired-end merging:
ls -S1 . | grep "trimmed.R1" | parallel -j ${THREADS} ${VSEARCH}" --fastq_mergepairs {} --reverse {.}.R2 --log - --fastq_allowmergestagger --threads 1 --fasta_width 0 --fastq_maxdiffs "${FASTQ_MAXDIFFS}" --fastq_minovlen "${FASTQ_MINOVLEN}" --fastqout {.}.assembled.fastq"

#create reverse complement:
ls -S1 . | grep ".assembled.fastq" | parallel -j ${THREADS} ${VSEARCH}" --fastx_revcomp {} --threads 1 --fastq_ascii 33 --fasta_width 0 --log - --fastqout {.}.revcomp.fastq"

#concatenate both directions:
ls -1 . | grep ".assembled.fastq" | awk '{gsub("fastq","",$1); printf($1"bothdir_concat.fastq\t"$1"fastq\t"$1"revcomp.fastq\n")}' | while read a b c ; do cat $b $c > $a ; done

#filter and trim primer:
ls -S1 | grep "bothdir_concat.fastq" | parallel -j ${THREADS} "cat {} | "${CUTADAPT}" -g "${FORWARDPRIMER}" -e "${PRIMER_MISMATCH}" -O "${lenFP_CUT}" --discard-untrimmed - | "${CUTADAPT}" -a "${REVERSEPRIMER_RC}" -e "${PRIMER_MISMATCH}" -O "${lenRP_CUT}" --discard-untrimmed - > {.}.primer_cut.fastq"

#Feature filtering and sha1 relabeling:
ls -S1 | grep "primer_cut.fastq" | parallel -j ${THREADS} ${VSEARCH}" --threads 1 --fastq_ascii 33 --log - --fasta_width 0 --fastx_filter {} --fastq_maxee "${FASTQ_MAXEE}" --fastq_maxlen "${FASTQ_MAXLEN}" --fastq_minlen "${FASTQ_MINLEN}" --fastq_maxns "${FASTQ_MAXNS}" --relabel_sha1 --relabel_keep --fastqout {.}.feature_filtered.fastq"

#dereplication on sample level:
ls -S1 | grep "feature_filtered.fastq" | parallel -j ${THREADS} ${VSEARCH}" --derep_fulllength {} --threads 1 --log - --fasta_width 0 --relabel_sha1 --sizeout --output {.}.derep.fasta"

#detect chimera:
ls -1S | grep "derep.fasta" | parallel -j ${THREADS} ${VSEARCH}" --fasta_width 0 --threads 1 --log - --log - --uchime_denovo {} --fasta_score --chimeras {.}.chimeras_denovo.fasta"

#create chimera black list:
awk -F";" 'NR%2==1 {gsub(">","");print $1}' *derep.chimeras_denovo.fasta | sort -n | uniq -c | sort -nr | awk '{if ($1==1) print $2}' > chimera.list

#filter chimera from fastas files:
ls -1S | grep "derep.fasta" | parallel -j ${THREADS} "awk 'FNR==NR{a[\$1]=\$1; next}(!(substr(\$0,2,index(\$0,\";\")-2) in a) && (\$0 ~ />/)){if (\$0 ~ /^>/) print \$0; getline; print}' chimera.list {} > {.}.non_chimeras_denovo.fasta"

#Min sample size filter
for i in *"${WORKFLOW_SUFFIX}.fasta"; do cnt=`awk -F"[;=]" 'BEGIN{cnt=0}{cnt+=$3}END{print cnt}' $i`; if [[ $cnt -lt ${MIN_SAMPLE_SIZE} ]]; then echo "remove "$i", too few sequences ("$cnt")"; rm $i; fi; done

#Dereplicate full study + adjust abundance information:
cat *derep.non_chimeras_denovo.fasta | ${VSEARCH} --threads ${THREADS} --derep_fulllength - --sizein --sizeout --log full_derep.log --fasta_width 0 --output - | sed 's/;size=/_/; s/;$//' > full_set_dereplicated.fasta

#Do swarming:
cat full_set_dereplicated.fasta | ${SWARM} -t ${THREADS} -d ${DISTANCE} -f -b ${F_BOUNDARY} -i ${S_STRUCT} -s ${S_STATS} -w ${S_SEEDS} > ${S_SWARM}

#filter singletons from seeds, swarm and stats file and create an amplicon names file:
awk -F"_" '{if (($0 ~ /^>/) && ($2 >= 2)) {print;getline;print} }' ${S_SEEDS} > ${S_SEEDS}".no.singletons"; awk '{if ( NF>1 || (match($0,"_1$") == zero)) print}' ${S_SWARM} > ${S_SWARM}.no.singletons; awk '{if ($2>1) print}' ${S_STATS} > ${S_STATS}.no.singletons; awk '{gsub(" ","\n");print}' ${S_SWARM}.no.singletons | awk -F"_" '{print $1}' > swarm.no.singletons

#Create amplicon tables for each sample/file
ls -1S | grep "${WORKFLOW_SUFFIX}.fasta" | parallel -j ${THREADS} "echo {} | awk '{sub(\"${WORKFLOW_SUFFIX}.fasta\",\"\");print}' > {.}.amplicon; awk '/^>/{gsub(/;size=/,\"\t\");gsub(\";\",\"\");gsub(\"^>\",\"\"); print}' {} | awk 'FNR==NR{a[\$1]=\$2;next} {if(a[\$1]!=\"\") print(a[\$1]); if (a[\$1]==\"\") print(\"0\") }' - swarm.no.singletons >> {.}.amplicon"

#add header to amplicon names list:
sed -i '1i amplicon' swarm.no.singletons

#create list of files to be merged:
slist="swarm.no.singletons "`ls -1 | grep ${WORKFLOW_SUFFIX}".amplicon"`

#merge amplicon names and all amplicon abundances to form an amplicon contingency table and sum-up rows (totalamplicon numbers):
paste $slist | parallel -j ${THREADS} -k -q --pipe awk '{if ($0 ~ /^amplicon/) {print} else {cnt=0; printf($1); for (i=2;i<=NF;i++) {cnt+=$i; printf("\t"$i)};printf("\t"cnt);printf("\n")}}' > ${AMPLICON_TABLE}"_unsorted"

#complete header of amplicon table:
sed -i '1s/$/\ttotal/' ${AMPLICON_TABLE}"_unsorted"

#Create sorted version of amplicon table:
head -1 ${AMPLICON_TABLE}"_unsorted" > ${AMPLICON_TABLE}; NUM_FIELDS=`head -2 ${AMPLICON_TABLE}"_unsorted" | tail -1 | awk '{print NF}'`; LC_ALL=C; awk 'NR > 1' ${AMPLICON_TABLE}"_unsorted" | sort -T . -k${NUM_FIELDS},${NUM_FIELDS}nr -k1,1d >> ${AMPLICON_TABLE}

#Create header for OTU table:
echo -e "#OTU\t$(head -n 1 "${AMPLICON_TABLE}")" > ${OTU_TABLE}

#Create OTU table:
cat ${S_STATS}.no.singletons | parallel -j ${THREADS_OTU} --pipe -l --block-size 100 --round-robin -q awk -v SWARM="${S_SWARM}.no.singletons" -v TABLE="${AMPLICON_TABLE}" 'BEGIN {FS = " "; while ((getline < SWARM) > 0) { swarms[$1] = $0 } FS = "\t"; while ((getline < TABLE) > 0) { table[$1] = $0 } } { seed = $3 "_" $4; n = split(swarms[seed], OTU, "[ _]"); for (i = 1; i < n; i = i + 2) { s = split(table[OTU[i]], abundances, "\t"); for (j = 1; j < s; j++) { samples[j] += abundances[j+1] } } printf "%s\t", $3; for (j = 1; j < s; j++) { printf "\t%s", samples[j] } printf "\n"; delete samples }' | awk '{print $NF,$0}' | sort -nr | cut -f2- -d" " | awk '{gsub("\t\t","\t");print NR"\t"$0}' >> ${OTU_TABLE}

#Taxonomic assignment of OTU representatives:
${MOTHUR} "#set.dir(output=.);classify.seqs(fasta="${S_SEEDS}".no.singletons, reference="${REF_USED_FP}"/"${REF_DBS}"_"${FORWARDPRIMER}"_"${REVERSEPRIMER_RC}"_"${PRIMER_MISMATCH_REF}"_"${lenFP_CUT_REF}"_"${lenRP_CUT_REF}".fasta, taxonomy="${REF_USED_FP}"/"${REF_DBS}"_"${FORWARDPRIMER}"_"${REVERSEPRIMER_RC}"_"${PRIMER_MISMATCH_REF}"_"${lenFP_CUT_REF}"_"${lenRP_CUT_REF}".tax, processors="${THREADS}",cutoff="${RDP_CUTOFF}", probs=F);get.current(); rename.file(taxonomy=current,new=swarm."${REF_DBS}"_${FORWARDPRIMER}_${REVERSEPRIMER_RC}_${PRIMER_MISMATCH_REF}_${lenFP_CUT_REF}_${lenRP_CUT_REF}.wang.taxonomy,shorten=false);"
